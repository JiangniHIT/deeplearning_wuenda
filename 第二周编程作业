import numpy as np
import matplotlib.pyplot as plt
import h5py
from lr_utils import load_dataset

train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes=load_dataset()
index =1
plt.imshow(train_set_x_orig[index])

print("train_set_y:"+str(train_set_y))

print("y=" + str(train_set_y[:,index]) +", it's a "+classes[np.squeeze(train_set_y[:,index])].decode("utf-8")+" picture")

m_train = train_set_y.shape[1] #训练集里图片的数量。 
m_test = test_set_y.shape[1] #测试集里图片的数量。 
num_px = train_set_x_orig.shape[1] #训练、测试集里面的图片的宽度和高度（均为64x64）。 

print("训练集的数量：m_train=" + str(m_train))
print("训练集图片的维数：" + str(train_set_x_orig.shape))
print("训练集标签的维数：" + str(train_set_y.shape))

train_set_x_flatten=train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T#将训练集的维度降低并转置 最后变成12288行，209列
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

#标准化数据集
train_set_x=train_set_x_flatten/255
test_set_x=test_set_x_flatten/255

#使用sigmoid计算做出预测
def sigmoid(z):
    """
    参数：
        z 任何大小的标量或numpy数组
        
    返回：
        s sigmoid(z)
    """
    s=1/(1+np.exp(-z))
    return s

print("====================测试sigmoid====================")
print ("sigmoid(0) = " + str(sigmoid(0)))
print ("sigmoid(9.2) = " + str(sigmoid(9.2)))

#初始化参数W b
def initialize_with_zeros(dim):
    w = np.zeros(shape = (dim,1))
    b = 0
    #使用断言来确保我要的数据是正确的
    assert(isinstance(b,float) or isinstance(b,int))
    return (w,b)

def propagate(w,b,X,Y):
    """
    实现前向和后向传播的成本函数及其梯度
    参数：
        w - 权重，大小不等的数组(num_px*num_px*3,1)
        b - 偏差，一个标量
        X - 矩阵类型为(num_px*num_px*3,训练数量)
        Y - 矩阵维度为（1，训练数据数量)
    返回：
        cost - 逻辑回归的负对数似然成本
        dw - 相对于w的损失梯度，与w形状相同
        db - 相对于b的损失梯度，与b形状相同
    """
    m=X.shape[1]
    #正向传播
    A=sigmoid(np.dot(w.T,X)+b)
    cost=-(1/m)*np.sum(Y*np.log(A)+(1-Y)*(np.log(1-A)))
    #反向传播
    dw = (1 / m) * np.dot(X, (A - Y).T)
    db=(1/m)*np.sum(A-Y)
    
    #使用断言确保数据是正确的
    assert(dw.shape==w.shape)
    assert(db.dtype==float)
    cost=np.squeeze(cost)
    assert(cost.shape==())
    
    #创建一个字典，把dw和db保存起来
    grads={
        "dw": dw,
        "db":db
    }
    return (grads,cost)

print("====================测试propagate====================")
#初始化一些参数
w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])
grads, cost = propagate(w, b, X, Y)
print("dw="+str(grads["dw"]))
print ("db = " + str(grads["db"]))
print ("cost = " + str(cost))

def optimize(w , b , X , Y , num_iterations , learning_rate , print_cost = False):
    """ 
    此函数通过运行梯度下降算法来优化w和b
    
    参数：
        w - 权重，大小不等的数组（num_px * num_px * 3，1） 
        b - 偏差，一个标量 
        X - 维度为（num_px * num_px * 3，训练数据的数量）的数组。 
        Y - 真正的“标签”矢量（如果非猫则为0，如果是猫则为1），矩阵维度为(1,训练数据的数量) 
        num_iterations - 优化循环的迭代次数 
        learning_rate - 梯度下降更新规则的学习率 
        print_cost - 每100步打印一次损失值 
        
    返回：
        params - 包含权重w和偏差b的字典 
        grads - 包含权重和偏差相对于成本函数的梯度的字典 
        成本 - 优化期间计算的所有成本列表，将用于绘制学习曲线。
        
    提示：
    我们需要写下两个步骤并遍历他们：
        1.计算当前参数的成本和梯度，使用propagate()
        2.使用w和b的梯度下降法更新参数
    """
    costs=[]
    for i in range(num_iterations):
        grads, cost = propagate(w, b, X, Y)
        dw=grads["dw"]
        db=grads["db"]
        
        w = w-learning_rate*dw
        b = b-learning_rate*db
        
        #记录成本
        if i%100 ==0:
            costs.append(cost)
        #打印成本数据
        if (print_cost) and (i%100 ==0):
            print("迭代的次数: %i ， 误差值： %f" % (i,cost))
            
    params={
        "w":w,
        "b":b
    }
    grads={
        "dw": dw,
        "db":db
    }
    return(params,grads,costs)

print("====================测试optimize====================")
w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])
params , grads , costs = optimize(w , b , X , Y , num_iterations=100 , learning_rate = 0.009 , print_cost = False)
print ("w = " + str(params["w"]))
print ("b = " + str(params["b"]))
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))

def predict(w,b,X):
    """
    使用学习逻辑回归参数logistic （w，b）预测标签是0还是1，

    参数：
        w  - 权重，大小不等的数组（num_px * num_px * 3，1）
        b  - 偏差，一个标量
        X  - 维度为（num_px * num_px * 3，训练数据的数量）的数据

    返回：
        Y_prediction  - 包含X中所有图片的所有预测【0 | 1】的一个numpy数组（向量）

    """
    m=X.shape[1]
    Y_prediction=np.zeros((1,m))
    w=w.reshape(X.shape[0],1)
    
    #预测猫在图片中出现的概率
    A=sigmoid(np.dot(w.T,X)+b)
    for i in range(A.shape[1]):
        Y_prediction[0,i]=1 if A[0,i]>0.5 else 0
    #使用断言
    assert(Y_prediction.shape == (1,m))
    return Y_prediction

print("====================测试predict====================")
w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])
print("predictions = " + str(predict(w, b, X)))

#将以上函数整合到model()中，届时只需要调用model()
def model(X_train , Y_train , X_test , Y_test , num_iterations = 2000 , learning_rate = 0.5 , print_cost = False):
    w,b=initialize_with_zeros(X_train.shape[0])
    parameters,grads,costs=optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)
    w,b=parameters["w"],parameters["b"]
    #预测测试、训练集的例子
    Y_prediction_test=predict(w,b,X_test)
    Y_prediction_train=predict(w,b,X_train)
    #打印训练后的准确性
    print("训练集准确性：", format(100-np.mean(np.abs(Y_prediction_train-Y_train))*100),"%")
    print("测试集准确性：", format(100-np.mean(np.abs(Y_prediction_test-Y_test))*100),"%")
    
    d={
        "costs":costs,
        "Y_prediction_test":Y_prediction_test,
        "Y_prediction_train":Y_prediction_train,
        "w":w,
        "b":b,
        "learning_rate":learning_rate,
        "num_iterations":num_iterations
    }
    return d
print("====================测试model====================") 
d=model(train_set_x,train_set_y,test_set_x,test_set_y,num_iterations = 2000 , learning_rate = 0.005 , print_cost = True)
    
